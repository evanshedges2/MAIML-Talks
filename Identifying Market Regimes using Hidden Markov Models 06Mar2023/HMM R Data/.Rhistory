}
# Test with sorted dataset from above
q1_finder(sorted_dataset)
# Function that given an ordered list (ascending), returns the 3rd quartile value
q3_finder <- function(list_to_analyze) {
return(median(list_to_analyze[1:ceiling(length(list_to_analyze)/2)]))
}
# Test with sorted dataset from above
q3_finder(sorted_dataset)
primes = data(primes)
data("primes")
library(UsingR)
install.packages("UsingR", dependencies = TRUE)
library(UsingR)
data("primes")
force(primes)
data("primes")
isprime <- function(num){
for (i in [2:cieling(sqrt(num))]){
library(UsingR)
data("primes")
isprime <- function(num){
for (i in 2:cieling(sqrt(num))){
if (num %% i == 0){
return(FALSE)
}
}
return(TRUE)
}
prime_count = 0
for (i in 1:length(primes)-2) {
if (isprime(i) && isprime(i+2)){
prime_count = prime_count + 1
}
}
library(UsingR)
data("primes")
isprime <- function(num){
for (i in 2:ceiling(sqrt(num))){
if (num %% i == 0){
return(FALSE)
}
}
return(TRUE)
}
prime_count = 0
for (i in 1:length(primes)-2) {
if (isprime(i) && isprime(i+2)){
prime_count = prime_count + 1
}
}
library(UsingR)
data("primes")
isprime <- function(num){
print(num)
for (i in 2:ceiling(sqrt(num))){
if (num %% i == 0){
return(FALSE)
}
}
return(TRUE)
}
prime_count = 0
for (i in 1:length(primes)-2) {
if (isprime(i) && isprime(i+2)){
prime_count = prime_count + 1
}
}
library(UsingR)
data("primes")
isprime <- function(num){
for (i in 2:ceiling(sqrt(num))){
if (num %% i == 0){
return(FALSE)
}
}
return(TRUE)
}
prime_count = 0
for (i in 1:(length(primes)-2)) {
if (isprime(i) && isprime(i+2)){
prime_count = prime_count + 1
}
}
data("alltime.movies")
lines(density(faithful$waiting))
faithful
lines(density(faithful$waiting))
plot.new
lines(density(faithful$waiting))
plot(faithful)
lines(density(faithful$waiting))
plot(movies)
nym.2002
nym
UsingR(nym)
library(UsingR)
nym2002
nym.2002
hist(nym.2002$time)
hist(nym.2002$time, bins=21)
hist(nym.2002$time, length.out=21)
hist(nym.2002[['time']], breaks = seq(min(nym.2002[['time']]), max(nym.2002[['time']]), length.out = 21))
hist(nym.2002$age)
hist(nym.2002$age, length.out=21)
hist(nym.2002$age, nbins=20)
hist(nym.2002$age, 20)
hist(nym.2002$age, nbins=20)
hist(nym.2002$age, nbins=20)
warnings()
hist(nym.2002$age, nbins=20)
hist(nym.2002$age, breaks=20)
hist(nym.2002$age, breaks=20)
hist(nym.2002$age, breaks=10)
hist(nym.2002$age, breaks=50)
hist(nym.2002$age, breaks=30)
boxplot(nym.2002$age)['out']
v = boxplot(nym.2002$age)['out']
View(v)
View(v)
v[1]
v
length(v[1])
length(v['out'])
v[1][1]
v[2]
v[[1]]
v[[1]][1]
length(v[11])
length(v[[1]])
v[,1]
v[1,]
v[[,1]]
v[[1,]]
w = v[1]
w[1]
rm(w)
data("alltime.movies")
force(alltime.movies)
data("movies.txt")
data("/movies.txt")
force(alltime.movies)
url = 'https://web.ma.utexas.edu/users/pgoetz/classes/m358k/data/mmc/movies.txt'
movies = read.table(url, header=T, sep='\t')
rm(alltime.movies)
rm(v)
View(movies)
View(movies)
sales = c(94, 97, 95, 94, 92)
names(sales) = c('Gru', 'Agnes', 'Margo', 'Edith', 'Victor')
barplot(sales, Main='Sales', ylab='Thousands')
barplot(sales, main='Sales', ylab='Thousands')
barplot(sales, main='Sales', ylab='Thousands', ylib=c(90, 97))
barplot(sales, main='Sales', ylab='Thousands', ylim=c(90, 97), xpd=FALSE)
par(mfrow(c(1,2)))
barplot(sales, main='Sales', ylab='Thousands', ylim=c(90, 97))
barplot(sales, main='Sales', ylab='Thousands', ylim=c(90, 97), xpd=FALSE)
install.packages('ggcorrplot')
url = 'https://web.ma.utexas.edu/users/pgoetz/classes/m358k/data/mmc/movies.txt'
movies = read.table(url, header=T, sep='\t')
moviesCor = cor(movies[,c(4, 5,9)])
require(reshape2)
require(scales)
moviesMelt = melt(moviesCor, varnames=c('x', 'y'), value.name='Correlation')
moviesMelt = moviesMelt[order(moviesMelt$Correlation)]
View(moviesMelt)
library(ggplot2)
ggplot(moviesMelt, aes(x=x, y=y))+geom_tile(aes(fill=Correlation))
median(moviesMelt$Correlation)
mean(moviesMelt$Correlation)
install.packages('reshape2')
install.packages("reshape2")
install.packages('scales')
install.packages("scales")
url = 'https://web.ma.utexas.edu/users/pgoetz/classes/m358k/data/mmc/movies.txt'
movies = read.table(url, header=T, sep='\t')
moviesCor = cor(movies[,c(4, 5,9)])
require('reshape2')
require('scales')
moviesMelt = melt(moviesCor, varnames=c('x', 'y'), value.name='Correlation')
moviesMelt = moviesMelt[order(moviesMelt$Correlation)]
url = 'https://web.ma.utexas.edu/users/pgoetz/classes/m358k/data/mmc/movies.txt'
movies = read.table(url, header=T, sep='\t')
moviesCor = cor(movies[,c(4, 5,9)])
require(reshape2)
require(scales)
moviesMelt = melt(moviesCor, varnames=c('x', 'y'), value.name='Correlation')
moviesMelt = moviesMelt[order(moviesMelt$Correlation)]
url = 'https://web.ma.utexas.edu/users/pgoetz/classes/m358k/data/mmc/movies.txt'
movies = read.table(url, header=T, sep='\t')
moviesCor = cor(movies[,c(4, 5,9)])
moviesCor
order(moviesMelt$Correlation)
moviesMelt = melt(moviesCor, varnames=c('x', 'y'), value.name='Correlation')
moviesMelt
moviesMelt = moviesMelt[order(moviesMelt$Correlation)]
moviesMelt = moviesMelt[,order(moviesMelt$Correlation)]
View(moviesMelt)
moviesMelt = moviesMelt[order(moviesMelt$Correlation),]
moviesMelt
ggplot(moviesMelt, aes(x=x, y=y))+geom_tile(aes(fill=Correlation))
median(moviesMelt$Correlation)
mean(moviesMelt$Correlation)
prob = 0
for (k in 0:37) {
prob = prob + choose(100, k) * (0.5)^100
}
prob*100
prob = 0
for (k in 0:450) {
prob = prob + choose(1000, k) * (0.5)^1000
}
prob
prob*100
prob = 0
for (k in 0:450) {
prob = prob + choose(1000, k) * (0.45)^k * (0.55)^(1000-k)
}
prob
choose(1000, 500) * 0.5^1000
1-pnorm(5, mean=0, sd=1, lower.tail=TRUE)
1/(1-pnorm(5, mean=0, sd=1, lower.tail=TRUE))
pnorm(37, 1000*0.5, sqrt(1000*0.5*.5))
pnorm(37, 100*0.5, sqrt(100*0.5*.5))
pnorm(370, 1000*0.5, sqrt(1000*0.5*.5))
n = 10
sample_normal_dataset = rnorm(n, mean=0, sd=1)
sorted_dataset = sort(sample_normal_dataset)
# Function that given an ordered list (ascending), returns the 1st quartile value
q1_finder <- function(list_to_analyze) {
return(median(list_to_analyze[1:floor(length(list_to_analyze)/2)]))
}
# Test with sorted dataset from above
q1_finder(sorted_dataset)
# Function that given an ordered list (ascending), returns the 3rd quartile value
q3_finder <- function(list_to_analyze) {
return(median(list_to_analyze[ceiling(length(list_to_analyze)/2):length(list_to_analyze)]))
}
# Test with sorted dataset from above
q3_finder(sorted_dataset)
library(UsingR)
data("primes")
# Using a for loop
twin_prime_count = 0
for (i in 1:(length(primes)-1)) {
if (primes[i+1] == (primes[i]+2)){
twin_prime_count = twin_prime+count + 1
}
}
library(UsingR)
data("primes")
# Using a for loop
twin_prime_count = 0
for (i in 1:(length(primes)-1)) {
if (primes[i+1] == (primes[i]+2)){
twin_prime_count = twin_prime_count + 1
}
}
# Using vector difference method and summing "TRUE" values where the difference in subsequent primes is 2
sum(primes[-1] - primes[-length(primes)] == 2)
data("alltime.movies")
boxplot(alltime.movies['Gross'], ylab='$M Gross')
gross_vector = alltime.movies[['Gross']]
gross_vector = sort(gross_vector)
q1 = summary(alltime.movies[,'Gross'])[['1st Qu.']]
q3 = summary(alltime.movies[,'Gross'])[['3rd Qu.']]
iqr = q3 - q1
outlier_count = 0
outlier_count = sum(gross_vector < (q1 - 1.5 * iqr))
outlier_count = outlier_count + sum(gross_vector > (q3 + 1.5 * iqr))
print(outlier_count)
# This method of using the upper/lower fences gives us a total of 5 outliers, which is the same as displayed in the box plot
summary(gross_vector)
fivenum(gross_vector)
# The 5 number summary does not tell us how many outliers there are, it only gives us the quartiles and mean.
url = 'https://web.ma.utexas.edu/users/pgoetz/classes/m358k/data/mmc/movies.txt'
movies = read.table(url, header=T, sep='\t')
# Capture Quartiles
rev_q1 = summary(movies[['USRevenue']])[['1st Qu.']]
rev_q3 = summary(movies[['USRevenue']])[['3rd Qu.']]
opinion_q1 = summary(movies[['Opinion']])[['1st Qu.']]
opinion_q3 = summary(movies[['Opinion']])[['3rd Qu.']]
# 4a: Command below prints the titles of the "winner" movies who had revenue and opinion > the 3rd quartile of each, respectively
winner_subset = subset(movies, select=c(Title, USRevenue, Opinion), subset=(USRevenue > rev_q3 & Opinion > opinion_q3))
winner_subset['Title']
# 4b: Command below prints the titles of the "loser" movies who had revenue and opinion < the 1st quartile of each, respectively
loser_subset = subset(movies, select=c(Title, USRevenue, Opinion), subset=(USRevenue < rev_q1 & Opinion < opinion_q1))
loser_subset['Title']
pie(winner_subset[['USRevenue']], labels = winner_subset[['Title']], main='It feels good to win!', sub='Evans')
data("nym.2002")
# 6a
View(nym.2002)
fast_marathon = subset(nym.2002, subset=(time < 180))
print(cat(100 * (nrow(fast_marathon) / nrow(nym.2002)), "% of marathoners finished in under 3 hours"))
# 6b
total_min = quantile(nym.2002[['time']], 0.1)
hours = floor(total_min / 60)
minutes = total_min %% 60
cat("The time to beat to be in the top 10% is: ", hours, ' hours and ', minutes, ' minutes.')
total_min = quantile(nym.2002[['time']], 0.25)
hours = floor(total_min / 60)
minutes = total_min %% 60
cat("The time to beat to be in the top 25% is: ", hours, ' hours and ', minutes, ' minutes.')
total_min = quantile(nym.2002[['time']], 0.75)
hours = floor(total_min / 60)
minutes = total_min %% 60
cat("The time to beat to be above the bottom 25% is: ", hours, ' hours and ', minutes, ' minutes.')
# 6c
hist(nym.2002[['time']])
hist(nym.2002[['time']], breaks = seq(min(nym.2002[['time']]), max(nym.2002[['time']]), length.out = 21))
# there is a small amount of information gained by doubling the number of bins, we can see the tail is much longer/thinner than we would expect given the first histogram.
# 6d
# I would not expect this data to be symmetric because the NYC marathon is an exclusive event and I would expect many of the participants to be somewhat close to the limits of human performance. This would mean we would see a clustering towards the faster end of the spectrum and fewer people having slower times.
# The data is positively (right) skewed, as expected.
# 6e
nymtime_q1 = summary(nym.2002[,'time'])[['1st Qu.']]
nymtime_q3 = summary(nym.2002[,'time'])[['3rd Qu.']]
nym_iqr = q3 - q1
outlier_count = 0
outlier_count = sum(nym.2002[['time']] < (nymtime_q1 - 1.5 * nym_iqr))
outlier_count = outlier_count + sum(nym.2002[['time']] > (nymtime_q3 + 1.5 * nym_iqr))
cat('There are ', outlier_count, ' outliers in the 2002 NY Marathon dataset according to time based on the upper/lower fence method')
# 6f
boxplot(nym.2002$age)["out"]
# This command lists all the outliers of the participants based on age based on the box plot.
sample_dist_10 = function(x) dnorm(x, mean=19, sd=5.2/sqrt(10))
sample_dist_20 = function(x) dnorm(x, mean=19, sd=5.2/sqrt(20))
sample_dist_100 = function(x) dnorm(x, mean=19, sd=5.2/sqrt(100))
curve(sample_dist_10, 13, 26, col='green', ylim=c(0, 0.8), ylab="Sample distribution function", main="Sample Distributions for n=10(g), 20(b), and 100(p)")
curve(sample_dist_20, 13, 26, col='blue', add=TRUE)
curve(sample_dist_100, 13, 26, col='purple', add=TRUE)
abline(h=0)
url = 'https://web.ma.utexas.edu/users/pgoetz/classes/m358k/data/mmc/movies.txt'
movies = read.table(url, header=T, sep='\t')
moviesCor = cor(movies[,c(4, 5,9)])
require(reshape2)
require(scales)
moviesMelt = melt(moviesCor, varnames=c('x', 'y'), value.name='Correlation')
moviesMelt = moviesMelt[order(moviesMelt$Correlation), ]
library(ggplot2)
ggplot(moviesMelt, aes(x=x, y=y))+geom_tile(aes(fill=Correlation))
median(moviesMelt$Correlation)
mean(moviesMelt$Correlation)
x=seq(-5, 5, .001)
plot(x, dnorm(x), type='1')
plot(x, dnorm(x, 0, 1), type='1')
dnorm(x, mean=0, sd=1)
y=dnorm(x, mean=0, sd=1)
plot(x, y, type='1')
plot(x, y)
lines(x, y, col='blue')
plot(x, dnorm(x, 0.5, 2))
lines(x, y, col='blue')
lines(x, dt(x), col='red')
dt(x)
dt(x, 120)
dt(x, 10)
lines(x, dt(x, 10), col='red')
plot()
plot(x)
plot(x, x)
graphics.off()
lines(x, dt(x, 10))
plot.new
lines(x, dt(x, 10))
plot.new()
lines(x, dt(x, 10))
plot(x, dt(x, 5), type='l')
lines(x, dt(x, 10))
lines(x, dt(x, 20), col='red')
y = pnorm(x[2:length(x)], 64.5, 2.5) - pnorm(x[1:length(x)-1], 64.5, 2.5)
plot(x[2:length(x)], y, type='l')
x = seq(-5, 5, 100)
y = pnorm(x[2:length(x)], 64.5, 2.5) - pnorm(x[1:length(x)-1], 64.5, 2.5)
plot(x[2:length(x)], y, type='l')
x = seq(-5, 5, .001)
y = pnorm(x[2:length(x)], 64.5, 2.5) - pnorm(x[1:length(x)-1], 64.5, 2.5)
plot(x[2:length(x)], y, type='l')
x = seq(-5, 5, .1)
y = pnorm(x[2:length(x)], 64.5, 2.5) - pnorm(x[1:length(x)-1], 64.5, 2.5)
plot(x[2:length(x)], y, type='l')
pnorm(71, 64.5, 2.5) - pnorm(69, 64.5, 2.5)
y = dchisq(x, 2)
plot(x, y, type='l')
lines(x, dchisq(x, 3), type='l')
x = seq(0, 5, .01)
plot(x, dchisq(x, 2), type='l')
plot(x, dchisq(x, 3), type='l')
plot(x, dchisq(x, 2), type='l')
lines(x, dchisq(x, 3), type='l')
lines(x, dchisq(x, 4), type='l')
lines(x, dchisq(x, 5), type='l')
x = seq(0, 10, .01)
plot(x, dchisq(x, 2), type='l')
lines(x, dchisq(x, 3), type='l')
lines(x, dchisq(x, 4), type='l')
lines(x, dchisq(x, 5), type='l')
lines(x, dchisq(x, 10), type='l')
plot(x, dchisq(x, 100), type='l')
plot(x, rnorm(100, 19. 5.2))
rnorm(100, 19, 5.2)
sampleit = function(n, popdist){
sample_averages = rep(0, 1)
for(i in 1:5000){
x = sample(popdist, n, replace=T)
sample_averages[i] = sum(x) / n
}
return(sample_averages)
}
rep(0, 1)
rep(0, 2)
x=5
x[1] = 2
x[2] = 4
x=5
x[1]=2
x=5
x[2]=4
pdist = 1:6
hist(pdist, breaks=100)
x = sampleit(2, pdist)
hist(x, breaks=100)
hist(sampleit(5, pdist), breaks=100)
hist(sampleit(50, pdist), breaks=100)
hist(sampleit(500, pdist), breaks=100)
hist(sampleit(500, pdist), breaks=10)
hist(sampleit(500, pdist), breaks=50)
hist(sampleit(5000, pdist), breaks=50)
hist(sampleit(5000, pdist), breaks=100)
hist(sampleit(50000, pdist), breaks=100)
hist(sampleit(5000, pdist), breaks=100)
hist(sampleit(5000, pdist), breaks=100)
hist(sampleit(5000, pdist), breaks=100)
hist(sampleit(50000, pdist), breaks=100)
pdist = c(rep(1, 30), rep(2, 15), rep(3, 10), rep(4, 5), rep(5, 1), rep(6,20))
hist(sampleit(5000, pdist), breaks=100)
hist(sampleit(50, pdist), breaks=100)
hist(sampleit(5, pdist), breaks=100)
hist(sampleit(2, pdist), breaks=100)
hist(sampleit(100000, pdist), breaks=100)
yellowt = c(63,76,85,92,76,65,82,85,69,68,78,69,79,87,79,94,100,81,73,88,95,98,94,51,99,81,80)
bluet = c(56,100,67,66,45,45,61,85,77,65,97,82,85,70,79)
tests = c(yellowt, bluet)
sampindex = sample(1:42,15)
bluesample=[sampindex]
bluesample=tests[sampindex]
yellowsample=tests[-sampindex]
resamp = rep(0,1)
for (k in 1:5000)
{
sampindex = sample(1:42,15)
bluesample = tests[sampindex]
yellowsample = tests[-sampindex]
resamp[k] = mean(yellowsample) - mean(bluesample)
}
hist(resampe, breaks=100)
hist(resamp, breaks=100)
prob = length(resamp[resamp>9]) / length(resamp)
t.test(yellowt, bluet)
prob = length(resamp[resamp>=9]) / length(resamp)
spy_df = read.csv(file='SPY 5sec bars.xlsx', header=TRUE)
setwd("~/Desktop/Markov Chain Presentation/Markov Chains Talk/R Data")
spy_df = read.csv(file='SPY 5sec bars.xlsx', header=TRUE)
spy_df = read.csv(file='SPY 5sec bars.xlsx', header=TRUE)
spy_df = read.csv(file='SPY 5sec bars.csv', header=TRUE)
spy_df = read.csv(file='SPY 5sec bars.csv', header=TRUE)
spy_df = read.csv(file='SPY 5sec bars.csv', header=TRUE)
log_return = log(spy_df[['Close']][2:length(spy_df[['close']])] /
spy_df[['Close']][1:length(spy_df[['close']])-1])
View(spy_df)
spy_df = read.csv(file='SPY 5sec bars.csv', header=TRUE)
log_return = log(spy_df[['Close']][2:length(spy_df[['close']])] /
spy_df[['Close']][1:length(spy_df[['close']])-1])
model = hmm(data=log_return, nstates=2)
library(hmmr)
model = hmm(data=log_return, nstates=2)
View(spy_df)
log_return = log(spy_df[['close']][2:length(spy_df[['close']])] /
spy_df[['close']][1:length(spy_df[['close']])-1])
model = hmm(data=log_return, nstates=2)
summary(model)
spy_df = read.csv(file='SPY 5sec bars.csv', header=TRUE)
returns = spy_df[['close']][2:length(spy_df[['close']])] -
spy_df[['close']][1:length(spy_df[['close']])-1]
model = hmm(data=log_return, nstates=2)
summary(model)
m2 <- lca(returns, nclasses=2)
hist(returns)
hist(returns, bins=10)
help(hist())
help(hist)
hist(returns, breaks=20)
hist(returns, breaks=30)
hist(returns, breaks=40)
hist(returns, breaks=50)
hist(returns, breaks=50)
hist(returns, breaks=60)
hist(returns, breaks=100)
m2 <- lca(returns, nclasses=2)
summary(m2)
model = hmm(data=returns, nstates=2)
summary(model)
filtering_probs = posterior(model, type='filtering')[,1]
plot(filtering_probs, type='l', col = 'blue', axes = FALSE, xlab = "", ylab = "")
plot(filtering_probs, type='l', col = 'blue', axes = FALSE, xlab = "", ylab = "")
par(new = TRUE)
plot(spy_df[['close']], type='l', main='S&P 500 from January 2018 through January 2023',
ylab='$SPY', lwd=2)
plot(filtering_probs, type='l', col = 'blue', axes = FALSE, xlab = "", ylab = "")
par(new = TRUE)
plot(spy_df[['close']], type='l', ylab='$SPY', lwd=2)
print(filtering_probs[length(filtering_probs)])
